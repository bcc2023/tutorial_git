1 May - 7 May

Thusrday

Friday

Saturday

Sunday
(Done)Reading survey papers
(In-progress) find out how excactly structral bias/ geometric input features are incorporated into transformer (Graphormer)
(In-progress) running https://github.com/Microsoft/Graphormer
(In-queue) what is token in tokenGT? why not have tokenGT with a modified attention? (why pure)
(In-queue) some theory: ARE MORE LAYERS BENEFICIAL TO GRAPH TRANSFORMERS?


8 May - 15 May

Monday
(Done) Graphormer: bring Graph informtaion/bias to transformer
(Done) tokenGT: pure self-attention to avoid old issues from message passing
(In-progress) some theoretical analysis - Graphormerï¼šhow it can genralize to other popular GNN
(In-progress/abort) could not clone a sub module? https://github.com/Microsoft/Graphormer
(In-progress) some theoretical analysis - why tokenGT still powerful
(In-queue) ARE MORE LAYERS BENEFICIAL TO GRAPH TRANSFORMERS?

Tuesday
(Done) revisiting attention is all you need 
(Done) some theoretical analysis: "bringing GNN to transformer" transformer can express GNN well by careful construction
(In-progress) benchmark: (invariant Graph Network) IGN and k-wl test
(In-progress) benchmark: GNN variants are special case of graph transformer
(In-progress) ARE MORE LAYERS BENEFICIAL TO GRAPH TRANSFORMERS?

Wednesday

Thusrday

Friday

Saturday

Sunday